# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import pandas as pd
import numpy as np
import os
import certifi
import time
from openai import AzureOpenAI
from dotenv import load_dotenv, find_dotenv

certifi.where()
_ = load_dotenv(find_dotenv())

# Set the environment variables for Snowflake timeouts
os.environ['SNOWFLAKE_NETWORK_TIMEOUT'] = '7200'  # 2 hours
os.environ['SNOWFLAKE_QUERY_TIMEOUT'] = '7200'    # 2 hours
os.environ['AZURE_OPENAI_API_KEY'] = '......................'

# Initialize OpenAI API using environment variables for security
client = AzureOpenAI(
    api_version="2023-07-01-preview",
    azure_endpoint="https://oai-predictive.openai.azure.com/",
)

# Set the chunk size for reading the large dataset
chunk_size = 2000000  # Adjust as needed based on memory constraints

# Define the randomized datasets
Transaction_clean_temp = ................Dataset("TEMP_RANDOM_PROJECT_DATA_CLEANED_SQL")
RANDOM_CATEGORY_MASTER_LIST = ................Dataset("RANDOM_CATEGORY_MASTER_LIST")

# Define known patterns and their corresponding cleansed names and categories
known_patterns = {
    'amzn mktp': ('Amazon', 'E-Commerce'),
    'amazoncom': ('Amazon', 'E-Commerce'),
    'cash app': ('Cash App', 'Financial Services')
}

# Counter for records sent to OpenAI
records_sent_to_openai = 0

def get_completion(prompt, model="gpt-4o"):
    messages = [{"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0  # this is the degree of randomness of the model's output
    )
    return response

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
def process_sub_chunk(sub_chunk, processed_businesses):
    global records_sent_to_openai  # To modify the global variable inside this function

    # Extract business names and SIC codes into a dictionary
    business_dict = {row['FINAL_BUSINESS_NAME']: row['BUSINESS_SIC_CODE'] for _, row in sub_chunk.iterrows()}

    business_names = list(business_dict.keys())
    if len(business_names) == 0:
        return None

    new_rows = []
    remaining_business_names = []

    for business_name in business_names:
        business_name_lower = business_name.lower()

        # Check if the business name matches any known patterns
        matched = False
        for pattern, (cleansed_name, category) in known_patterns.items():
            if pattern in business_name_lower:
                business_sic_code = business_dict.get(business_name)
                new_rows.append([business_name, business_sic_code, cleansed_name, category])
                processed_businesses.add(business_name)
                matched = True
                break

        if not matched and business_name not in processed_businesses:
            remaining_business_names.append(business_name)

    # Batch size for sending records to OpenAI
    batch_size = 20  # Adjust this number based on your needs

    # Batch remaining business names and send to OpenAI
    if remaining_business_names:
        for i in range(0, len(remaining_business_names), batch_size):
            batch = remaining_business_names[i:i + batch_size]
            prompt = """You will be provided with a transaction description.
            You have to act as a transaction mapper for banks and credit unions.
            Associate the transaction ACH data to the company generic name and industry category in a uniform pattern.
            Give the full name of the company in the company generic name and as generic as possible.
            Return the output in structured table format columns include transaction_desc, company_generic_name, industry_category.
            Return only the table in the output.

            ''' \n{0}
            """.format("\n".join(batch))
            #print(prompt)
            try:
                time.sleep(5)  # To respect API rate limits
                response = get_completion(prompt)
                print(response.choices[0].message.content)
                # Check if the response or content is None before processing
                if response is None or response.choices[0].message.content is None:
                    raise ValueError("Response from API is None")
                    #print(response.choices[0].message.content)

                lines = response.choices[0].message.content.strip().split("\n")[2:]
                for line in lines:
                    fields = [i.strip() for i in line.split("|")]
                    if len(fields) >= 3:
                        transaction_desc, business_generic, business_category = fields[1], fields[2], fields[3]
                        business_sic_code = business_dict.get(transaction_desc)
                        new_rows.append([transaction_desc, business_sic_code, business_generic, business_category])
                        processed_businesses.add(transaction_desc)

                # Update the counter after processing the batch
                records_sent_to_openai += len(batch)
                print(f"Records sent to OpenAI so far: {records_sent_to_openai}")

            except Exception as e:
                print(f"Error processing batch: {type(e).__name__} - {str(e)}")

    if new_rows:
        return pd.DataFrame(new_rows, columns=['BUSINESS_NAME', 'BUSINESS_SIC_CODE', 'BUSINESS_CLEANSED_NAME', 'BUSINESS_CATEGORY'])

    return None

def remove_null_sic_code_duplicates(df):
    # Separate rows with null and non-null BUSINESS_SIC_CODE
    null_sic_df = df[df['BUSINESS_SIC_CODE'].isnull()]
    non_null_sic_df = df[df['BUSINESS_SIC_CODE'].notnull()]

    # Drop duplicates in each group, keeping the first occurrence
    null_sic_df = null_sic_df.drop_duplicates(subset=['BUSINESS_NAME'], keep='first')
    non_null_sic_df = non_null_sic_df.drop_duplicates(subset=['BUSINESS_NAME'], keep='first')

    # Combine the results back together
    df = pd.concat([null_sic_df, non_null_sic_df])

    return df

def process_chunk(chunk, processed_businesses):
    # Refresh the master list and add to the processed_businesses set
    RANDOM_CATEGORY_MASTER_LIST_df = RANDOM_CATEGORY_MASTER_LIST.get_dataframe(columns=['BUSINESS_NAME'])
    processed_businesses.update(RANDOM_CATEGORY_MASTER_LIST_df['BUSINESS_NAME'].tolist())

    chunk = chunk.drop_duplicates(subset=['FINAL_BUSINESS_NAME'])
    chunk = chunk[~chunk['FINAL_BUSINESS_NAME'].isin(processed_businesses)]
    chunk = chunk.dropna(subset=['FINAL_BUSINESS_NAME'])

    if chunk.empty:
        print("Chunk is empty after filtering. Skipping.")
        return None

    sub_chunks = np.array_split(chunk, int(len(chunk) / 1000) + 1)

    new_business_rows = []

    for sub_chunk in sub_chunks:
        result_df = process_sub_chunk(sub_chunk, processed_businesses)
        if result_df is not None and not result_df.empty:
            new_business_rows.append(result_df)

    if not new_business_rows:
        print("No new data to process in this chunk.")
        return None

    # Concatenate and remove duplicates with null SIC codes
    new_business_df = pd.concat(new_business_rows, ignore_index=True).drop_duplicates(subset=['BUSINESS_NAME'])
    new_business_df = remove_null_sic_code_duplicates(new_business_df)

    return new_business_df

def read_data_with_retries(dataset, chunk_size, max_retries=3):
    """Read data from a dataset with retry mechanism."""
    for retry in range(max_retries):
        try:
            return dataset.iter_dataframes(columns=['FINAL_BUSINESS_NAME', 'BUSINESS_SIC_CODE'], chunksize=chunk_size)
        except Exception as e:
            print(f"Attempt {retry + 1} failed: {e}")
            time.sleep(10)  # Wait before retrying
    raise Exception("Failed to read data after multiple attempts.")

# Initialize a set to keep track of processed businesses
processed_businesses = set()

# Iterate through the dataset in chunks
chunks = read_data_with_retries(Transaction_clean_temp, chunk_size)
for idx, chunk in enumerate(chunks, start=1):
    print(f"Processing chunk {idx}...")
    new_business_df = process_chunk(chunk, processed_businesses)
    if new_business_df is not None and not new_business_df.empty:
        # Uncomment below to write to the dataset if needed
        # RANDOM_CATEGORY_MASTER_LIST.write_with_schema(new_business_df)
        print(f"Processed chunk {idx} and obtained {len(new_business_df)} new business records.")
    else:
        print(f"No new business records found in chunk {idx}.")

print("Processing complete.")
print(f"Total records sent to OpenAI: {records_sent_to_openai}")
