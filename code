import pandas as pd
import numpy as np
import os
import certifi
import time
from openai import AzureOpenAI
from dotenv import load_dotenv, find_dotenv

certifi.where()
_ = load_dotenv(find_dotenv())

# Set timeouts for network operations (configurable)
NETWORK_TIMEOUT = '7200'  # 2 hours
QUERY_TIMEOUT = '7200'    # 2 hours

# Load environment variables
os.environ['SNOWFLAKE_NETWORK_TIMEOUT'] = NETWORK_TIMEOUT
os.environ['SNOWFLAKE_QUERY_TIMEOUT'] = QUERY_TIMEOUT
os.environ['AZURE_OPENAI_API_KEY'] = '......................'

# Initialize OpenAI API
client = AzureOpenAI(
    api_version="2023-07-01-preview",
    azure_endpoint="https://oai-predictive.openai.azure.com/",
)

# Set chunk size for processing data in segments (adjustable)
CHUNK_SIZE = 2000000  # Adjust based on available memory

# Define new, abstracted dataset names
transaction_data = ................Dataset("TEMP_MODIFIED_PROJECT_DATA_CLEANED_SQL")
business_category_master = ................Dataset("MODIFIED_CATEGORY_MASTER_LIST")

# Generalized known pattern mappings for businesses
general_patterns = {
    'amzn mktp': ('Amazon', 'E-Commerce'),
    'amazoncom': ('Amazon', 'E-Commerce'),
    'cash app': ('Cash App', 'Financial Services')
}

# Global counter for tracking records processed
records_processed = 0

def request_openai_completion(prompt, model="gpt-4o"):
    """Request response from OpenAI using chat completion."""
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0  # Control the randomness
    )
    return response

# --------------------------------------------------------------------------------
def process_transaction_chunk(chunk, processed_businesses):
    """Processes a chunk of transaction data to map to businesses."""
    global records_processed
    
    business_dict = {row['FINAL_BUSINESS_NAME']: row['BUSINESS_SIC_CODE'] for _, row in chunk.iterrows()}
    business_names = list(business_dict.keys())
    
    if not business_names:
        return None

    new_rows = []
    unmatched_businesses = []

    # Match known patterns or add to unmatched
    for business_name in business_names:
        normalized_name = business_name.lower()
        matched = False
        
        for pattern, (cleaned_name, category) in general_patterns.items():
            if pattern in normalized_name:
                business_sic = business_dict.get(business_name)
                new_rows.append([business_name, business_sic, cleaned_name, category])
                processed_businesses.add(business_name)
                matched = True
                break

        if not matched and business_name not in processed_businesses:
            unmatched_businesses.append(business_name)

    # Handle remaining unmatched businesses via OpenAI (batched requests)
    if unmatched_businesses:
        batch_size = 20  # Batch size for OpenAI requests
        for i in range(0, len(unmatched_businesses), batch_size):
            batch = unmatched_businesses[i:i + batch_size]
            prompt = generate_openai_prompt(batch)
            
            try:
                time.sleep(5)  # Respect API rate limits
                response = request_openai_completion(prompt)
                if response:
                    process_openai_response(response, business_dict, new_rows, processed_businesses)

                # Update processed counter
                records_processed += len(batch)
                print(f"Records processed so far: {records_processed}")
                
            except Exception as e:
                print(f"Error with batch processing: {type(e).__name__} - {str(e)}")

    if new_rows:
        return pd.DataFrame(new_rows, columns=['BUSINESS_NAME', 'BUSINESS_SIC_CODE', 'BUSINESS_CLEANSED_NAME', 'BUSINESS_CATEGORY'])
    
    return None

def generate_openai_prompt(business_names):
    """Generates a dynamic prompt for sending to OpenAI."""
    return """You will be provided with a description of transactions.
    Your task is to map each to a generic business name and an industry category.
    Use the most general name possible and avoid brand-specific terms.
    Return a structured table with columns: transaction_description, company_generic_name, industry_category.

    ''' \n{0}
    """.format("\n".join(business_names))

def process_openai_response(response, business_dict, new_rows, processed_businesses):
    """Processes OpenAI response and updates the new_rows list."""
    try:
        lines = response.choices[0].message.content.strip().split("\n")[2:]
        for line in lines:
            fields = [field.strip() for field in line.split("|")]
            if len(fields) >= 3:
                transaction_desc, generic_name, category = fields[1], fields[2], fields[3]
                business_sic = business_dict.get(transaction_desc)
                new_rows.append([transaction_desc, business_sic, generic_name, category])
                processed_businesses.add(transaction_desc)
    except Exception as e:
        print(f"Error parsing OpenAI response: {type(e).__name__} - {str(e)}")

def clean_duplicates(df):
    """Remove duplicate entries based on SIC code or missing SIC codes."""
    null_sic_df = df[df['BUSINESS_SIC_CODE'].isnull()].drop_duplicates(subset=['BUSINESS_NAME'], keep='first')
    non_null_sic_df = df[df['BUSINESS_SIC_CODE'].notnull()].drop_duplicates(subset=['BUSINESS_NAME'], keep='first')
    
    return pd.concat([null_sic_df, non_null_sic_df])

def handle_chunk(chunk, processed_businesses):
    """Manages the chunk processing and filtering."""
    business_category_master_df = business_category_master.get_dataframe(columns=['BUSINESS_NAME'])
    processed_businesses.update(business_category_master_df['BUSINESS_NAME'].tolist())

    chunk = chunk.drop_duplicates(subset=['FINAL_BUSINESS_NAME']).dropna(subset=['FINAL_BUSINESS_NAME'])
    chunk = chunk[~chunk['FINAL_BUSINESS_NAME'].isin(processed_businesses)]
    
    if chunk.empty:
        print("No new data to process.")
        return None

    sub_chunks = np.array_split(chunk, int(len(chunk) / 1000) + 1)
    new_business_rows = [process_transaction_chunk(sub_chunk, processed_businesses) for sub_chunk in sub_chunks if sub_chunk is not None]

    if not new_business_rows:
        print("No new business records in this chunk.")
        return None
    
    new_business_df = pd.concat(new_business_rows, ignore_index=True).drop_duplicates(subset=['BUSINESS_NAME'])
    return clean_duplicates(new_business_df)

def retry_data_loading(dataset, chunk_size, retries=3):
    """Retries data loading for better resilience."""
    for attempt in range(retries):
        try:
            return dataset.iter_dataframes(columns=['FINAL_BUSINESS_NAME', 'BUSINESS_SIC_CODE'], chunksize=chunk_size)
        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            time.sleep(10)
    raise Exception("Failed to load data after several attempts.")

# Start processing
processed_businesses = set()
chunks = retry_data_loading(transaction_data, CHUNK_SIZE)

for idx, chunk in enumerate(chunks, start=1):
    print(f"Processing chunk {idx}...")
    new_business_df = handle_chunk(chunk, processed_businesses)
    if new_business_df is not None and not new_business_df.empty:
        print(f"Processed {len(new_business_df)} new business records in chunk {idx}.")
    else:
        print(f"No new business records found in chunk {idx}.")

print("Processing complete.")
print(f"Total records processed: {records_processed}")
